# === SERVICE(LLM Microservice) === (might need to have a volume)
services:
  llm-api:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    command: >
      sh -c "uvicorn app.main:app --host 0.0.0.0 --port 8000"
    ports:
      - "8001:8000"
    depends_on:
      - rd2
    networks:
      - llm-app-network
      - shared-bridge
    restart: always

  llm-worker:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    command: >
      sh -c "celery -A celery_worker.celery_app worker -l info"
    depends_on:
      - rd2
      - llm-api
    networks:
      - llm-app-network
      - shared-bridge
    volumes:
      - media_volume:/app/media
    restart: always

  llm-infer:
    build:
      context: .
      dockerfile: Dockerfile.cuda  # <-- the second Dockerfile with CUDA
    environment:
      - STUDY_FRIEND_CUDA=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    networks:
      - llm-app-network
      - shared-bridge
    volumes:
      - media_volume:/app/media
    depends_on:
      - rd2
    restart: always

  rd2:
    image: redis:alpine
    command: redis-server --appendonly yes --appendfsync everysec
    volumes:
      - rd2-data:/data
    networks:
      - llm-app-network
    restart: always

volumes:
  rd2-data:
    external: true
  media_volume:
    external: true

networks:
  llm-app-network:
    external: true
  shared-bridge:
    external: true
